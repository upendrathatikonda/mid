<html>
    <head>
        <title>
            mid1
        </title>
        <link rel="stylesheet" href="midcss.css">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
    </head>
    <script>
          $(function(){
                    $(".iot").click(function(){
                        $("#iotdef").fadeToggle();
                        $("#physicaldesign").fadeToggle();
                        $("#sensors").fadeToggle();
                        $("#mqtt").fadeToggle();
                        $("#coap").fadeToggle();
                    });
                });

                $(function(){
                    $("#st").click(function(){
                       
                        $("#mental").fadeToggle();
                        $("#pt").fadeToggle();
                        $("#versus").fadeToggle();
                        $("#consequences").fadeToggle();
                        $("#dfs").fadeToggle();
                    });
                });
                $(function(){
                    $(".cd").click(function(){
                        $("#phase").fadeToggle();
                        $("#inputbuffer").fadeToggle();
                    });
                });

                $(function(){
                    $(".ml").click(function(){
                        $("#bias").fadeToggle();
                        $("#roc").fadeToggle();
                        $("#overfitting").fadeToggle();
                        $("#underfitting").fadeToggle();
                        $("#kfold").fadeToggle();
                    });
                });

                $(function(){
                    $("#iotdef").click(function(){
                        $(".para1").fadeToggle();
                    });
                });
                $(function(){
                    $("#physicaldesign").click(function(){
                        $(".para2").fadeToggle();
                    });
                });
                $(function(){
                    $("#sensors").click(function(){
                        $(".para3").fadeToggle();
                    });
                });
                $(function(){
                    $("#mqtt").click(function(){
                        $(".para4").fadeToggle();
                    });
                });
                $(function(){
                    $("#coap").click(function(){
                        $(".para5").fadeToggle();
                    });
                });

                $(function(){
                    $("#inputbuffer").click(function(){
                        $(".para7").fadeToggle();
                    });
                });

                $(function(){
                    $("#phase").click(function(){
                        $(".para6").fadeToggle();
                    });
                });

                $(function(){
                    $("#bias").click(function(){
                        $(".para11").fadeToggle();
                    });
                });

                $(function(){
                    $("#underfitting").click(function(){
                        $(".para10").fadeToggle();
                    });
                });

                $(function(){
                    $("#overfitting").click(function(){
                        $(".para9").fadeToggle();
                    });
                });

                $(function(){
                    $("#roc").click(function(){
                        $(".para8").fadeToggle();
                    });
                });
                $(function(){
                    $("#consequences").click(function(){
                        $(".para17").fadeToggle();
                    });
                });
                $(function(){
                    $("#mental").click(function(){
                        $(".para13").fadeToggle();
                    });
                });
                $(function(){
                    $("#dfs").click(function(){
                        $(".para16").fadeToggle();
                    });
                });
                $(function(){
                    $("#versus").click(function(){
                        $(".para15").fadeToggle();
                    });
                });
                $(function(){
                    $("#pt").click(function(){
                        $(".para14").fadeToggle();
                    });
                });
    </script>
    <body>
        <div class="fullpage">
            <div class="iot">
        <h1>
            IOT 
        </h1>
        </div>
        <div class="midiot">
        <div class="sub-heading"id="iotdef">
        <h2>
            IOT DEFINITION
        </h2>
        </div>
        <div class="para1">
        <p>
            The Internet of Things (IoT) describes the network of physical objects—“things”—that are embedded with sensors, software, and other technologies for the purpose of connecting and exchanging data with other devices and systems over the internet. 

        </p>
        </div>
        <div class="sub-heading"id="physicaldesign">
            <h2>
            PHYSICAL DESIGN OF IOT
        </h2>
        </div>
        <div class="para2">
            <p>
                The physical design of an IoT system is referred to as the Things/Devices and protocols that are used to build an IoT system. All these things/Devices are called Node Devices and every device has a unique identity that performs remote sensing, actuating, and monitoring work. and the protocols that are used to establish communication between the Node devices and servers over the internet.
            </p>
            <h3>
                Things/Devices
            </h3>
            <p>
                Things/Devices are used to build a connection, process data, provide interfaces, provide storage, and provide graphics interfaces in an IoT system. All these generate data in a form that can be analyzed by an analytical system and program to perform operations and used to improve the system. 

for example temperature sensor that is used to analyze the temperature generates the data from a location and is then determined by algorithms.
                <img src="https://1.bp.blogspot.com/-PaDoquzETRU/YCESKL3w9wI/AAAAAAAAADE/8kXEpuvZGBkc9Q1XMck91-9s5xdlyLZswCNcBGAsYHQ/w640-h346/physical-design-of-iot-things.png" caption="devices in IoT(Internet of things)">
                <h3>Connectivity</h3>
Devices like USB hosts and ETHERNET are used for connectivity between the devices and the server.
<h3>
Processor
</h3>
A processor like a CPU and other units are used to process the data. these data are further used to improve the decision quality of an IoT system.
<h3>
Audio/Video Interfaces
</h3>
An interface like HDMI and RCA devices is used to record audio and videos in a system.
<h3>Input/Output interface
</h3>
    To give input and output signals to sensors, and actuators we use things like UART, SPI, CAN, etc.
<h3>
Storage Interfaces
</h3>
Things like SD, MMC, and SDIO are used to store the data generated from an IoT device.
<p>
Other things like DDR and GPU are used to control the activity of an IoT system.
   </p>         
</p>
<h3>
    IoT Protocols
</h3>
<p>
    These protocols are used to establish communication between a node device and a server over the internet. it helps to send commands to an IoT device and receive data from an IoT device over the internet. we use different types of protocols that are present on both the server and client side and these protocols are managed by network layers like application, transport, network, and link layer.</p>
    <img src="https://1.bp.blogspot.com/-9ymOjrDg_2A/YCEVLHuIA2I/AAAAAAAAADQ/fSsZUJw1qIwgJEUGRvYvCarra-oX75tqwCNcBGAsYHQ/w400-h281/physical-design-of-iot-protocols.png">
<h3>Application Layer protocol</h3><p>
In this layer, protocols define how the data can be sent over the network with the lower layer protocols using the application interface. these protocols include HTTP, WebSocket, XMPP, MQTT, DDS, and AMQP protocols.
</p><h3>
HTTP</h3><p>
Hypertext transfer protocol is a protocol that presents an application layer for transmitting media documents. it is used to communicate between web browsers and servers. it makes a request to a server and then waits till it receives a response and in between the request server does not keep any data between the two requests. 
</p><h3>
WebSocket</h3><p>
This protocol enables two-way communication between a client and a host that can be run on an untrusted code in a controlled environment. This protocol is commonly used by web browsers.
</p><h3>
MQTT</h3><p>
It is a machine-to-machine connectivity protocol that was designed as a publish/subscribe messaging transport. and it is used for remote locations where a small code footprint is required.
</p><h3>
Transport Layer</h3><p>
This layer is used to control the flow of data segments and handle error control. also, these layer protocols provide end-to-end message transfer capability independent of the underlying network.
</p><h3>
TCP</h3><p>
The transmission control protocol is a protocol that defines how to establish and maintain a network that can exchange data in a proper manner using the internet protocol.
</p><h3>
    UDP</h3><p>
a user datagram protocol is part of an internet protocol called the connectionless protocol. this protocol is not required to establish the connection to transfer data.
</p><h3>
Network Layer</h3><p>This layer is used to send datagrams from the source network to the destination network. we use IPv4 and IPv6 protocols as host identification that transfers data in packets.
</p><h3>
    IPv4</h3><p>
This is a protocol address that is a unique and numerical label assigned to each device connected to the network. an IP address performs two main functions host and location addressing. IPv4 is an IP address that is 32-bit long.
</p><h3>
IPv6</h3><p>
It is a successor of IPv4 that uses 128 bits for an IP address. it is developed by the IETF task force to deal with long-anticipated problems.
</p><h3>
Link Layer</h3><p>Link-layer protocols are used to send data over the network’s physical layer. it also determines how the packets are coded and signaled by the devices.
</p><h3>
Ethernet
</h3><p>
It is a set of technologies and protocols that are used primarily in LANs. it defines the physical layer and the medium access control for wired ethernet networks.
</p><h3>
    WiFi</h3>
    <p>
It is a set of LAN protocols and specifies the set of media access control and physical layer protocols for implementing wireless local area networks.</p>
        </div>
        <div class="sub-heading"id="sensors">
            <h2>
                Sensors&Actuators
            </h2>
        </div>
        <div class="para3">
            <h3>What are IoT Sensors? </h3></br>
            <p>IoT sensors are pieces of hardware that detect changes in an environment and collect data. They’re the pieces of an IoT ecosystem that bridge the digital world to the physical world. IoT sensors may detect things like temperature, pressure, and motion, and if they are connected to a network, they share data with the network. </p>
            <h3>14 Types of IoT Sensors</h3>
            <p>There are many different types of sensors, and they come in different shapes and sizes. Here are 14 of the most common types and uses of sensors.</p>
            <ul>
           <li> <b>1. Temperature Sensors</b></li>
           <li><b>2. Proximity Sensors</b></li>
            <li><b>3. Pressure Sensors</b></li>
            <li><b>4. Water Quality Sensors</b></li>
            <li><b>5. Chemical and Gas Sensors</b></li>
            <li><b>6. Infrared Sensors</b></li>
            <li><b>7. Smoke Sensors</b></li>
            <li><b>8. Motion Sensors</b></li>
            <li><b>9. Level Sensors</b></li>
            <li><b>10. Image Sensors</b></li>
            <li><b>11. Humidity Sensors</b></li>
            <li><b>12. Accelerometer Sensors</b></li>
            <li><b>13. Gyroscope Sensors</b></li>
            <li><b>14. Optical Sensors</b></li></ul>
            <h3>
                what are actuators in IOT
            </h3>
            <p>An actuator is a device that converts energy into motion. It does this by taking an electrical signal and combining it with an energy source. In an IoT system, the actuator can act on data collected by sensors to create an outcome as determined by the chosen settings of the user.</p>
            </div>
       <div class="sub-heading"id="mqtt">
        <h2>
            MQTT
        </h2>
       </div>
       <div class="para4">
        <p>
            MQTT (MQ Telemetry Transport) is a lightweight open messaging protocol that provides resource-constrained network clients with a simple way to distribute telemetry information in low-bandwidth environments. The protocol, which employs a publish/subscribe communication pattern, is used for machine-to-machine (M2M) communication.
        </p>
        <h3>
            MQTT Architecture
        </h3>
        <img src="https://static.javatpoint.com/tutorial/computer-network/images/mqtt-protocol3.png" alt="Architecture">
        <p>
            To understand the MQTT architecture, we first look at the components of the MQTT.
        </p>
        <ul>
            <li>Message</li>
            <li>Client</li>
            <li>Server or Broker</li>
            <li>Topic</li>
        </ul>
       </div> 
       <div class="sub-heading"id="coap">
        <h3>
            CoAP
        </h3>
       </div>    
       <div class="para5">
        <p>CoAP a customary client-server IoT protocol. It enables clients to make requests for web transfers as per the need of the hour. On the other hand, it also let supporting servers to respond to arriving requests. In summary, devices’ nodes in the IoT ecosystem are enabled to interact over through CoAP only.</p>
        <br>
        <p>CoAP and HTTP follow the same working procedure. However, CoAP attains its functionality via asynchronous transactions (using UDP). It utilizes the POST, GET, PUT, and DELETE calls. </p>
        <br>
        <h4>Layers in CoAP</h4>
        <p>there are two different layers :</p>
        <ul>
            <li>Request/Response</li>
            <li>Message</li>
        </ul>
        <img src="https://miro.medium.com/v2/resize:fit:640/0*P-Yb4ZnauPOeoEnt">
        <p>In this above diagram you can see there are two different layers that make CoAp protocol:Message and Request/Response.The Message layer deals with UDP and with asynchronous messages. The Request/Response layer manages request/response interaction based on request/response messages.</p>
       <p>CoAP Protocol supports four different message types:</p>
       <ul>
        <li>Confirmable</li>
        <li>Non-confirmable</li>
        <li>Acknowledgment</li>
        <li>Reset</li>
       </ul> 
       </div>
        </div>
        </div>


<div class="fullpage">
    <div class="cd">
        <h1>CD</h1>
    </div>
    <div id="phase">
        <h2>1.Explain the phase of a compiler with block diagram.</h2>
    </div>
    <div class="para6">
        <p><b>Ans:</b>A compiler operates in phases. A phase is a logically interrelated operation that takes source program in one representation and produces output in another representation. The phases of a compiler are shown in below:</p>
        <p><b>1.Analysis phase</b>breaks up the source program into constituent pieces and creates an intermediate representation of the source program. Analysis of source program includes: lexical analysis, syntax analysis and semantic analysis.</p>
        <p><b>2.Synthasis phase</b>construct the desired target program from the intermediate representation. The synthesis part of compiler consists of the following phases: Intermediate code generation, Code optimization and Target code generation</p>
        <img src="https://tse3.mm.bing.net/th?id=OIP.Aph6qOTNg7DdjF8OH4RH0QHaGO&pid=Api&P=0" alt="compiler diagram">
        <p><b>1.Lexical Analysis</b><br>In this phase, lexical analyzer reads the source program and returns the tokens of the source program. Token is a sequence of characters that can be treated as a single logical entity (such as identifier, operators, keywords, constants etc.).
            <br>Example:<br>
                Input String: c = a + b * 3<br>
                Tokens: id1 = id2 + id3 * 3<br>
            </p>
            <p><b>2.syntax Analysis</b>In this phase, the syntax analyzer takes the token produced by lexical analyzer as input and generates a parse tree as output. In syntax analysis phase, the parser checks that the expression made by the token is syntactically correct or not, according to the rules that define the syntax of the source language.
                Example:<br>
                <img src="https://static.javatpoint.com/compiler/images/compiler-phases1.png"></p>
            <p><b>3.semantic Analysis</b>In this phase, the semantic analyzer checks the source program for semantic errors and collects the type information for the code generation. Semantic analyzer checks whether they form a sensible set of instructions in the programming language or not. Type-checking is an important part of semantic analyzer.
                Example:
                </p>
                <p><b>4.Intermediate Code Generator</b>If the program syntactically and semantically correct then intermediate code generator generates a simple machine independent intermediate language. The intermediate code should be generated in such a way that it can easily translated into the target machine code.
                    <br>Example:<br>
                        t1 = 3.0;<br>
                        t2 = id3 * t1;<br>
                        t3 = id2 + t2;<br>
                        id1 = t3;
                    </p>
            <p><b>5.Code optimization</b>It is used to improve the intermediate code so that the output of the program could run faster and takes less space. It removes the unnecessary lines of the code and arranges the sequence of statements in order to speed up the program execution without wasting resources.
                <br>Example:<br>
                    t2 = id3 * 3.0;<br>
                    id1 = id2 + t2;<br>
                </p>
                <p><b>6.Code Generation</b>Code generation is the final stage of the compilation process. It takes the optimized intermediate code as input and maps it to the target machine language.
                  <br>  Example:
                      <br>  MOV  R1, id3
                       <br> MUL  R1, #3.0
                       <br>MOV  R2, id2
                        <br>ADD  R1, R2
                        <br>MOV  id1, R1
                    </p>
                    <h3>Symbol Table</h3>
                    <p>Symbol tables are data structures that are used by compilers to hold information about source-program constructs. The information is collected incrementally by the  analysis phase of compiler and used by the synthesis phases to generate the target code. Entries in the symbol table contain information about an identifier such as its type, its position in storage, and any other relevant information.</p>
                    <h3>Error Handling</h3>
                    <p>Whenever an error is encountered during the compilation of the source program, an error handler is invoked. Error handler generates a suitable error reporting message regarding the error encountered.</p>
    </div>
    <div id="inputbuffer">
        <h2>
            2.Define token, pattern and lexeme with suitable example. How input buffering can be implemented for scanner, Explain?
        </h2>
    </div>
    <div class="para7">
        <p><b>Token:</b><br> is a sequence of characters that can be treated as a single logical entity. For example, Identifiers, Keywords, Operators, Constants etc.
           <br><b>Pattern:</b><br>
            A set of string in the input for which the same token is produced as output. This set of string is described by a rule called a pattern associated with the token. For example, “a letter followed by zero or more letters, digits, or underscores.”
            <br><b>Lexeme:</b><br>
            A lexeme is a sequence of characters in the source program that is matched by the pattern for a token.
            <br><b>Input Buffering:</b><br>
            Lexical analysis needs to look ahead several characters before a match can be announced. We have two buffer input scheme that is useful when look ahead is necessary.
            </p>
            <p>The lexical analyzer scans the input from left to right one character at a time. It uses two pointers begin ptr(bp) and forward ptr(fp) to keep track of the pointer of the input scanned. </p>
            <img src="https://media.geeksforgeeks.org/wp-content/uploads/20190401013238/Untitled-Diagram-211.png" >
            <h4>Initial Configuration</h4>
            <p>Initially both the pointers point to the first character of the input string as</p>
            <img src="https://media.geeksforgeeks.org/wp-content/uploads/20190401013001/Untitled-Diagram-115.png">
            <h4>input buffering</h4>
            <img src="https://media.geeksforgeeks.org/wp-content/uploads/20190401013638/Untitled-Diagram-36.png">
            <b>1.One Buffer Scheme</b><p>: In this scheme, only one buffer is used to store the input string but the problem with this scheme is that if lexeme is very long then it crosses the buffer boundary, to scan rest of the lexeme the buffer has to be refilled, that makes overwriting the first of lexeme.</p>
            <img src="https://media.geeksforgeeks.org/wp-content/uploads/20190401012723/Untitled-Diagram12.png">
            <br>
            <p>
            <b>2.Two Buffer Scheme:</b>
                To overcome the problem of one buffer scheme, in this method two buffers are used to store the input string. the first buffer and second buffer are scanned alternately. when end of current buffer is reached the other buffer is filled. the only problem with this method is that if length of the lexeme is longer than length of the buffer then scanning input cannot be scanned completely. Initially both the bp and fp are pointing to the first character of first buffer. Then the fp moves towards right in search of end of lexeme. as soon as blank character is recognized, the string between bp and fp is identified as corresponding token. to identify, the boundary of first buffer end of buffer character should be placed at the end first buffer. Similarly end of second buffer is also recognized by the end of buffer mark present at the end of second buffer. when fp encounters first eof, then one can recognize end of first buffer and hence filling up second buffer is started. in the same way when second eof is obtained then it indicates of second buffer. alternatively both the buffers can be filled up until end of the input program and stream of tokens is identified. This eof character introduced at the end is calling Sentinel which is used to identify the end of buffer.
            </p>
            <img src="https://media.geeksforgeeks.org/wp-content/uploads/20190401014427/Untitled-Diagram-43.png">
        </div>
</div>
<div class="fullpage">
    <div class="ml">
        <h1>
            ML
        </h1>
    </div>
    <div id="roc">
        <h2>ROC CURVE</h2>
    </div>
    <div class="para8">
        <p>A ROC (which stands for “receiver operating characteristic”) curve is a graph that shows a classification model performance at all classification thresholds. It is a probability curve that plots two parameters, the True Positive Rate (TPR) against the False Positive Rate (FPR), at different threshold values and separates a so-called ‘signal’ from the ‘noise.</p>
        <h3>How to Create a ROC Curve:</h3>
        <p>Once we’ve fit a logistic regression model, we can use the model to classify observations into one of two categories.</p><br><br>
        <p>For example, we might classify observations as either “positive” or “negative.”</p><br><br>
        <p>The true positive rate represents the proportion of observations that are predicted to be positive when indeed they are positive.</p><br><br>
        <p>Conversely, the false positive rate represents the proportion of observations that are predicted to be positive when they’re actually negative.</p><br><br>
        <p>When we create a ROC curve, we plot pairs of the true positive rate vs. the false positive rate for every possible decision threshold of a logistic regression model.</p><br><br>
        
    </div>
    <div id="overfitting">
        <h2>Overfitting</h2>
    </div>
    <div class="para9">
        <p>Overfitting occurs when our machine learning model tries to cover all the data points or more than the required data points present in the given dataset. Because of this, the model starts caching noise and inaccurate values present in the dataset, and all these factors reduce the efficiency and accuracy of the model. The overfitted model has low bias and high variance.</p>
        <br><br><p>The chances of occurrence of overfitting increase as much we provide training to our model. It means the more we train our model, the more chances of occurring the overfitted model.</p>
        <i>example:</i><p>The concept of the overfitting can be understood by the below graph of the linear regression output:</p>
        <img src="https://static.javatpoint.com/tutorial/machine-learning/images/overfitting-and-underfitting.png">
        <p>As we can see from the above graph, the model tries to cover all the data points present in the scatter plot. It may look efficient, but in reality, it is not so. Because the goal of the regression model to find the best fit line, but here we have not got any best fit, so, it will generate the prediction errors.</p>
        <h3>How to avoid the Overfitting in Model</h3>
        <p>Both overfitting and underfitting cause the degraded performance of the machine learning model. But the main cause is overfitting, so there are some ways by which we can reduce the occurrence of overfitting in our model.</p>
        <ul>
            <li>Cross-validation</li>
            <li>Training with more data</li>
            <li>Removing features</li>
            <li>Early stopping the training</li>
            <li>Regularization</li>
            <li>Ensembling</li>
        </ul>
    </div>
    <div id="underfitting">
        <h2>Underfitting</h2>
    </div>
    <div class="para10">
        <p>Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. As a result, it may fail to find the best fit of the dominant trend in the data.</p>
        <br>
        <br>
        <p>In the case of underfitting, the model is not able to learn enough from the training data, and hence it reduces the accuracy and produces unreliable predictions.</p>
        <br>
        <br>
        <p>An underfitted model has high bias and low variance.</p>
        <i>Example:</i><p>We can understand the underfitting using below output of the linear regression model:</p>
        <img src="https://static.javatpoint.com/tutorial/machine-learning/images/overfitting-and-underfitting2.png">
        <p>As we can see from the above diagram, the model is unable to capture the data points present in the plot.</p>
        <h3>How to avoid Underfitting</h3>
        <ul>
            <li>
                By increasing the training time of the model
            </li>
            <li>
                By increasing the number of features.
            </li>
        </ul>
    </div>
    <div id="bias">
        <h2>Bias-Variance Tradeoff</h2>
    </div>
    <div class="para11">
        <p>For any model, we have to find the perfect balance between Bias and Variance. This just ensures that we capture the essential patterns in our model while ignoring the noise present it in. This is called Bias-Variance Tradeoff. It helps optimize the error in our model and keeps it as low as possible. </p>
        <br>
        <p>An optimized model will be sensitive to the patterns in our data, but at the same time will be able to generalize to new data. In this, both the bias and variance should be low so as to prevent overfitting and underfitting.
        </p>
        <figure>
            <img src="https://www.simplilearn.com/ice9/free_resources_article_thumb/7-bullseye.JPG" alt="BUll's Eye graph for bias and variance" style="width:100%">
            <figcaption> Bull’s Eye Graph for Bias and Variance</figcaption>
          </figure>
        <p>The above bull’s eye graph helps explain bias and variance tradeoff better. The best fit is when the data is concentrated in the center, ie: at the bull’s eye. We can see that as we get farther and farther away from the center, the error increases in our model. The best model is one where bias and variance are both low.</p>
    </div>
    <div id="kfold">
        <h2>K-Fold Cross Validation</h2>
    </div>
    <div class="para12">
        <p>Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.
</p><p>
            The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.
       </p><p>     
            Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.
            </p><p>
            It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.</p>
            <p>In k-fold  the value of k in k-fold cross validation can be set to any number. However there are two approaches which are extremly popular .
                <ul>
                    <li>
                        K-10-fold cross validation
                    </li>
                    <li>Leave one out cross validation(LOOCV)</li>
                </ul>
            </p>
            <p><h3>k-10-fold cross validation</h3>10-fold cross-validation is by far the most popular
                approach. In this approach, for each of the 10-folds, each
                comprising of approximately 10% of the data, one of the
                folds is used as the test data for validating model
                performance trained based on the remaining 9 folds (or
                90% of the data). This is repeated 10 times, once for each
                of the 10 folds being used as the test data and the
                remaining folds as the training data.The average
                performance across all folds is being reported</p>
                <p><h3>LOOCV</h3>
                    Leave-one-out cross-validation (LOOCV) is an extreme
                    case of k-fold cross-validation using one record or data
                    instance at a time as a test data. This is done to maximize
                    the count of data used to train the model. It is obvious
                    that the number of iterations for which it has to be run is
                    equal to the total number of data in the input data set.
                    Hence, obviously, it is computationally very expensive
                    and not used much in practice.
                    </p>
                    <div id="s">
                        <h2>Difference between supervised and unsupervised and reinforcement</h2>
                    </div>
                    <div class="para12">
                        <table><tr>
                            <th>
                                Criteria
                            </th>
                            </tr>
                        </table>
                    </div>
</div>
</div>
<div class="fullpage">
<div id="st">
    <h1>ST</h1>
</div>
<div id="mental">
    <h2>Testers mental life</h2>
</div>

<div class="para13">
    <p style="border: 2px solid blue;">Phase 0: There’s no difference between testing and debugging. Other than in support of debugging, testing has no purpose.
<br>
        Phase 0 Thinking: Testing = Debugging</p>
        <p style="border:2px solid blue;">Phase 1:The purpose of testing is to show that the software works.
        <br>Phase 1 Thinking:The S/W Works</p>
        <p style="border:2px solid blue;">Phase 2: The purpose of testing is to show that the software doesn’t work.
<br>
            Phase 2 Thinking: The S/W Doesn’t Work</p>
            <p style="border:2px solid blue;">Phase 3: The purpose of testing is not to prove anything, but to reduce the perceived risk of the software not working to an acceptable value.
<br>
                Phase 3 Thinking: Test for Risk Reduction</p>
                <p style="border:2px solid blue;">Phase 4: Testing is not an act. It is a mental discipline that results in low-risk software without much testing effort.
<br>
                    Phase 4 Thinking: A State of Mind</p>
</div>
<div id="pt">
    <h2>Path testing</h2>
</div>
<div class="para14">
    <p>Path Testing is a method that is used to design the test cases. In path testing method, the control flow graph of a program is designed to find a set of linearly independent paths of execution. In this method Cyclomatic Complexity is used to determine the number of linearly independent paths and then test cases are generated for each path. 
    </p><p>
        It give complete branch coverage but achieves that without covering all possible paths of the control flow graph. McCabe’s Cyclomatic Complexity is used in path testing. It is a structural testing method that uses the source code of a program to find every possible executable path. </p>
        <img src="https://media.geeksforgeeks.org/wp-content/uploads/20190530113621/223.jpg">
        <ul>
            <li><b>Control Flow Graph:</b>Draw the corresponding control flow graph of the program in which all the executable paths are to be discovered.</li>
            <li><b>Cyclomatic Complexity:</b>After the generation of the control flow graph, calculate the cyclomatic complexity of the program using the following formul.</li>
            <li><b>Make Set:</b>Make a set of all the path according to the control flow graph and calculated cyclomatic complexity. The cardinality of set is equal to the calculated cyclomatic complexity.</li>
            <li><b>Create Test Cases:</b>Create test case for each path of the set obtained in above step.</li>
        </ul>
        <h3>Path selection criteria</h3>
        <ul>
           <li> There are many paths between the entry and exit of a typical routine.</li>
           <li> Every decision doubles the number of potential paths. And every loop multiplies the number of potential paths by the number of different iteration values possible for the loop.</li>
            <li>Defining complete testing:
                <ol>
            <li>Exercise every path from entry to exit</li>
            <li>Exercise every statement or instruction at least once</li>
            <li>Exercise every branch and case statement, in each direction at least once</li></ol></li>
            <li>If prescription 1 is followed then 2 and 3 are automatically followed. But it is impractical for most routines. It can be done for the routines that have no loops, in which it is equivalent to 2 and 3 prescriptions.</li>
        </ul>
        <b>
            example:
        </b>
        <img src="http://mcr.org.in/sureshmudunuri/stm/images/figures/path_selection_example.jpg">
        <h3>Rules for Path selection</h3>
        <ul><li>
Pick the simplest, functionally sensible entry/exit path.</li>
<li>Pick additional paths as small variation from previous paths. Pick paths that do not have loops rather than paths that do. Favor short paths that make sense over paths that don't.</li>
<li>Pick additional paths that have no obvious functional meaning only if it's necessary to provide coverage.</li>
<li>Be comfortable with your chosen paths. Play your hunches (guesses) and give your intuition free reign as long as you achieve C1+C2.</li>
<li>Don't follow rules slavishly (blindly) - except for coverage.</li>
        </ul>
</div>
<div id="versus">
    <h2>testing versus debugging </h2>
</div>
<div class="para15">
    <table><thead><tr><th>Testing</th><th>Debugging</th></tr></thead><tbody><tr><td><a href="https://www.geeksforgeeks.org/software-testing-basics/">Testing </a>is the process to find bugs and errors.</td><td><a href="https://www.geeksforgeeks.org/software-engineering-debugging/">Debugging</a> is the process of correcting the bugs found during testing.</td></tr><tr><td>It is the process to identify the failure of implemented code.</td><td>It is the process to give absolution to code failure.</td></tr><tr><td>Testing is the display of errors.</td><td>Debugging is a deductive process.</td></tr><tr><td>Testing is done by the tester.</td><td>Debugging is done by either programmer or the developer.</td></tr><tr><td>There is no need of design knowledge in the testing process.</td><td>Debugging can’t be done without proper design knowledge.</td></tr><tr><td>Testing can be done by insiders as well as outsiders.</td><td>Debugging is done only by insiders. An outsider can’t do debugging.</td></tr><tr><td>Testing can be manual or automated.</td><td>Debugging is always manual. Debugging can’t be automated.</td></tr><tr><td>It is based on different testing levels i.e. unit testing, integration testing, system testing, etc.</td><td>Debugging is based on different types of bugs.</td></tr><tr><td>Testing is a stage of the software development life cycle (SDLC).</td><td>Debugging is not an aspect of the software development life cycle, it occurs as a consequence of testing.</td></tr><tr><td>Testing is composed of the validation and verification of software.</td><td>While debugging process seeks to match symptoms with cause, by that it leads to error correction.</td></tr><tr><td>Testing is initiated after the code is written.</td><td>Debugging commences with the execution of a test case.</td></tr><tr><td>Testing process based on various levels of testing-system testing, integration testing, unit testing, etc.</td><td>Debugging process based on various types of bugs is present in a system.</td></tr></tbody></table>


    <style>
        table, td, th {
          border: 1px solid;
        }
        
        table {
          width: 100%;
          border-collapse: collapse;
        }
        </style>
</div>
<div id="consequences">
    <h2>Consequences of Bugs</h2>
</div>
<div class="para17">
    <p>
        The consequences of a bug can be measured in terms of human, rather than machine, Some consequences of a bug on a scale of one to ten are:
    </p>
    <ul><li>
        1. Mild: The symptoms of the bug offend us gently; a misspelled output or a misaligned printout
</li><li> Moderate: Outputs are misleading or redundant. The bug impacts the systems performance
</li><li>3. Annoying: The Systems behaviour, because of the bug is dehumanizing
Example: Names are truncated or arbitrarily modified
</li><li> Disturbing: It refuses to handle legitimate (authorized/legal) transactions.
Example: The ATM wont give money. Credit card is declared invalid
</li><li> Serious: It loses track of its transaction. Not just the transaction itself but the fact that the transaction occurred and accountability is lost
</li><li>6. Very Serious: The bug causes the system to do the wrong transactions. Instead of gaining a pay check, the system credits it to another account or converts deposits to withdrawals
</li><li>7. Extreme: The problems are not limited to a few users or to few transaction types. They are frequent and arbitrary, ( instead of occasionally infrequent) or for unusual case
</li><li>8. Intolerable: Long term unrecoverable corruption of the database occurs and the corruption is not easily discovered. Serious consideration is given to shutting the system down
</li><li>9. Catastrophic: The decision to shut down is taken out of our hands because the system fails
</li><li>10. Infectious: What can be worse than a failed system? One that corrupts other systems even though it does not fail in itself; that erodes the social physical environment; that melts reactors and starts a war
    </li></ul>
</div>
<div id="dfs">
    <h2>
        Data flow Testing strategies
    </h2>
</div>
<div class="para16">
    <p>STRATEGIES: The structural test strategies discussed below are based on the program's control flowgraph. They differ in the extent to which predicate uses and/or computational uses of variables are included in the test set. Various types of data flow testing strategies in decreasing order of their effectiveness are:
<ol><li>
        All - du Paths (ADUP): The all-du-paths (ADUP) strategy is the strongest data-flow testing strategy discussed here. It requires that every du path from every definition of every variable to every use of that definition be exercised under some test.
        <p>
        For variable X and Y:In Figure 3.9, because variables X and Y are used only on link (1,3), any test that starts at the entry satisfies this criterion (for variables X and Y, but not for all variables as required by the strategy).
        </p><p>
        For variable Z: The situation for variable Z (Figure 3.10) is more complicated because the variable is redefined in many places. For the definition on link (1,3) we must exercise paths that include subpaths (1,3,4) and (1,3,5). The definition on link (4,5) is covered by any path that includes (5,6), such as subpath (1,3,4,5,6, ...). The (5,6) definition requires paths that include subpaths (5,6,7,4) and (5,6,7,8).
        </p><p>
        For variable V: Variable V (Figure 3.11) is defined only once on link (1,3). Because V has a predicate use at node 12 and the subsequent path to the end must be forced for both directions at node 12, the all-du-paths strategy for this variable requires that we exercise all loop-free entry/exit paths and at least one path that includes the loop caused by (11,4). Note that we must test paths that include both subpaths (3,4,5) and (3,5) even though neither of these has V definitions. They must be included because they provide alternate du paths to the V use on link (5,6). Although (7,4) is not used in the test set for variable V, it will be included in the test set that covers the predicate uses of array variable V() and U.
        </p><p>        The all-du-paths strategy is a strong criterion, but it does not take as many tests as it might seem at first because any one test simultaneously satisfies the criterion for several definitions and uses of several different variables.</p>
        </li><li>
        All Uses Startegy (AU):The all uses strategy is that at least one definition clear path from every definition of every variable to every use of that definition be exercised under some test. Just as we reduced our ambitions by stepping down from all paths (P) to branch coverage (C2), say, we can reduce the number of test cases by asking that the test set should include at least one path segment from every definition to every use that can be reached by that definition.
        <p>
        For variable V: In Figure 3.11, ADUP requires that we include subpaths (3,4,5) and (3,5) in some test because subsequent uses of V, such as on link (5,6), can be reached by either alternative. In AU either (3,4,5) or (3,5) can be used to start paths, but we don't have to use both. Similarly, we can skip the (8,10) link if we've included the (8,9,10) subpath. Note the hole. We must include (8,9,10) in some test cases because that's the only way to reach the c use at link (9,10) - but suppose our bug for variable V is on link (8,10) after all? Find a covering set of paths under AU for Figure 3.11.
        </p><p>        All p-uses/some c-uses strategy (APU+C) : For every variable and every definition of that variable, include at least one definition free path from the definition to every predicate use; if there are definitions of the variables that are not covered by the above prescription, then add computational use test cases as required to cover every definition.
        </p><p>
        For variable Z:In Figure 3.10, for APU+C we can select paths that all take the upper link (12,13) and therefore we do not cover the c-use of Z: but that's okay according to the strategy's definition because every definition is covered. Links (1,3), (4,5), (5,6), and (7,8) must be included because they contain definitions for variable Z. Links (3,4), (3,5), (8,9), (8,10), (9,6), and (9,10) must be included because they contain predicate uses of Z. Find a covering set of test cases under APU+C for all variables in this example - it only takes two tests.
        </p><p>
        For variable V:In Figure 3.11, APU+C is achieved for V by (1,3,5,6,7,8,10,11,4,5,6,7,8,10,11,12[upper], 13,2) and (1,3,5,6,7,8,10,11,12[lower], 13,2). Note that the c-use at (9,10) need not be included under the APU+C criterion.
        </p></li><li>
        All c-uses/some p-uses strategy (ACU+P) : The all c-uses/some p-uses strategy (ACU+P) is to first ensure coverage by computational use cases and if any definition is not covered by the previously selected paths, add such predicate use cases as are needed to assure that every definition is included in some test.
<p>
        For variable Z: In Figure 3.10, ACU+P coverage is achieved for Z by path (1,3,4,5,6,7,8,10, 11,12,13[lower], 2), but the predicate uses of several definitions are not covered. Specifically, the (1,3) definition is not covered for the (3,5) p-use, the (7,8) definition is not covered for the (8,9), (9,6) and (9, 10) p-uses.
   </p><p>     
        The above examples imply that APU+C is stronger than branch coverage but ACU+P may be weaker than, or incomparable to, branch coverage.
        </p></li><li>
        All Definitions Strategy (AD) : The all definitions strategy asks only every definition of every variable be covered by atleast one use of that variable, be that use a computational use or a predicate use.
        <p>        For variable Z: Path (1,3,4,5,6,7,8, . . .) satisfies this criterion for variable Z, whereas any entry/exit path satisfies it for variable V.
</p><p>
        From the definition of this strategy we would expect it to be weaker than both ACU+P and APU+C.
       </p></li><li>
        All Predicate Uses (APU), All Computational Uses (ACU) Strategies : The all predicate uses strategy is derived from APU+C strategy by dropping the requirement that we include a c-use for the variable if there are no p-uses for the variable. The all computational uses strategy is derived from ACU+P strategy by dropping the requirement that we include a p-use for the variable if there are no c-uses for the variable.
        </p>
</div>
</div>
    </body>
</html>
